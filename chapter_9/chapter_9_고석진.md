# HTTP

## 웹 로봇

웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다.
웹 사이트를 돌아다니면서 컨텐츠를 가져오고, 하이퍼링크를 따라가고, 발견한 데이터를 처리한다.

## 9.1 크롤러와 크롤링 

웹 크롤러는 페이지를 순회하며 (페이지 내부 링크 포함) 재귀적으로 모든 데이터를 가져오는 방식으로 동작하는 로봇이다.

### 9.1.1 루트 집합

크롤러가 방문을 시작하는 URL 들의 초기 집합을 루트 집합이라고 부른다. 
일반적으로 좋은 루트 집합은 크고 인기있는 웹사이트, 새로 생성된 페이지들의 목록, 자주 링크되지 않는 페이지들의 목록이다.

### 9.1.2 링크 추추롸 상대 링크 정상화 

크롤러들은 간단한 HTML 파싱을 해서 이들 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요가 있다. 

### 9.1.3 순환 피하기 / 9.1.4 루프와 중복 

웹을 크롤링 할 때 무한루프에 빠지지 않도록 주의해야한다.
반보되는 순환을 피하기 위해서 반드시 어디를 방문했는지 알아야한다. 

- 순환은 크롤러를 루프에 빠뜨려서 꼼짝 못하게 만들 수 있다.
- 같은 페이지를 반복하여 가져온다면 웹 서버의 부담이 된다.

### 9.1.5 빵 부스러기의 흔적 

로봇은 어떤 URL 이 방문했던 곳인지 빠르게 결정하기 위해 검색 트리나 해시 테이블을 필요로 할 것 이다.
수억 개의 URL 은 많은 공간을 차지한다.

- 트리와 해시 테이블: 복잡한 로봇이라면 방문한 URL 을 추적하기 위해 검색 트리나 해시 테이블을 사용했을 수도 있다.
- 느슨한 존재 비트맵: 공간 사용을 최소화하기 위해, 대규모 크롤러들은 존재 비트 배열과 같은 느슨한 자료 구조를 사용한다.
- 체크 포인트: 로봇 프로그램이 갑작스럽게 중단될 경우를 대비해, 방문한 URL 의 목록이 디스크에 저장되었는지 확인한다.
- 파티셔닝: 웹이 성장하면서 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해졌다. 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 농장을 이용한다.

### 9.1.6 별칭과 로봇순환 

URL 이 별칭을 가질 수 있는 이상 어떤 페이지를 이전에 방문했었는지 말해주는게 쉽지 않을 때도 있다.
한 URL 이 또 다른 URL 에 대한 별칭이라면, 그 둘이 서로 달라 보이더라도 사실은 같은 리소스를 가리키고 있다.

### 9.1.7 URL 정규화하기 

웹 로봇은 URL 들을 표준 형식으로 '정규화' 함으로써 다른 URL 과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려 시도한다.

- 포트 번호가 명시되지 않았다면, 호스트 명에 ':80' 을 추가한다.
- 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
- # 태그들을 제거한다.

### 9.1.8 파일 시스템 링크 순환 

파일 시스템의 심벌릭 링크는 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 떄문에 에러를 유발 할 수 있다.

1. GET http://www.foo.com/index.html => /index.html => subdir/index.html 
2. http://www.foo.com/subdir/index.html => /index.html 반복 

### 9.1.10 루프와 중복 피하기

- URL 정규화: 표준 형태로 변환함으로써, 같은 리소스를 가리키는 중복된 URL 이 생기는 것을 일부 회피한다.
- 너비 우선 크롤링: 방문할 URL 들을 웹 사이트들 전체에 걸쳐 너비 우선으로 스케줄링하면 순환의 영향을 최소화할 수 있다.
- 스로틀링: 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한한다.
- URL 크기 제한: 일정 길이를 넘는 URL 의 크롤링을 거부 할 수 있다.
- URL/사이트 블랙리스트: 문제를 일으키는 사이트나 URL 이 발견될 때마다 블랙리스트에 추가한다.
- 패턴 발견: 파일시스템의 심벌리 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르는 경향있다. 반복되는 구성요소를 갖고 있는 URL 을 크롤링하는 것을 거절한다.
- 콘턴츠 지문: 로봇이 이전에 보았던 체크섬을 가진 페이지를 가져온다면 해당 페이지는 크롤링 하지 않는다.
- 사람 모니터링: 문제를 예방하기 위해 사람의 모니터링에 더욱 의존한다.

## 9.2 로봇의 HTTP

로봇 또한 HTTP 명세의 규칙을 지켜야한다. 

### 9.2.1 요청 헤더 식별하기

로봇들은 신원 식별 헤더 (User Agent HTTP 헤더) 를 구현하고 전송한다.
로봇 구현자들은 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤더를 사이트에게 보내주는 것이 좋다.

- User Agent: 서버에게 요청을 만든 로봇의 이름을 말해준다.
- From: 로봇의 사용자/관리자의 이메일 주소를 제공한다.
- Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다.
- Referer: 현재의 요청 URL 을 포함한 문서의 URL 을 제공한다.

### 9.2.2 가상 호스팅 

로봇 구현자들은 Host 헤더를 지원할 필요가 있다. Host 헤더를 포함하지 않으면 로봇이 어떤 URL 에 대해 잘못된 콘텐츠를 찾게 만든다.

### 9.2.3 조건부 요청

로봇이 검색하는 콘텐츠의 양을 최소화하는 것(변경이 일어났을때만 가져온다던가 .. )은 상당히 의미 있는 일이다.

### 9.2.4 응답 다루기 

웹 탐색이나 서버와의 상호작용을 해보려고 하는 로봇들은 여러 종류의 HTTP 응답을 다룰 줄 알 필요가 있다.

- 상태코드: 모든 로봇은 200, 404 와 같은 HTTP 상태 코드를 이해해야한다. 
- 엔터티: HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있다.

### 9.2.5 User-Agent 타겟팅

웹 사이트들은 여러 기능을 지원할 수 있도록 브라우저의 종류를 감지하여 그에 맞게 컨텐츠를 제공한다.
사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야한다.

## 9.3 부적절하게 동작하는 로봇들

- 폭주하는 로봇: 로봇은 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있다. 빠른 네트워크 위에서 동작한다. 
- 오래된 URL: 로봇들은 존재하지 않는 URL 에 대한 요청을 많이 보낼 수 있다. 존재하지 않는 페이지에 대한 접근 에러가 쌓일 수 있다.
- 길고 잘못된 URL: 크고 의미 없는 URL 을 요청 할 수 있다.
- 호기심이 지나친 로봇: 사적인 데이터에 대한 URL 을 얻어 데이터를 인터넷 검색엔진이나 어플맄이션을 통해 쉽게 접근할 수 있도록 만들 수도 있다.
- 동적 게이트웨이 접근: 로봇은 게이트웨이 애플리케이션의 콘텐츠에 대한 URL로 요청을 할수도 있다.

## 9.4 로봇 차단하기

로봇의 접근을 제어하는 정보를 저장하는 파일의 이름을 따서 robots.txt 라고 부른다.
xxx/index.html 에 대한 정보를 다운받으려고 할 때 먼저 접근 권한을 확인하기 위해 robots 파일을 검사한다.

### 9.4.1 로봇 차단 표준 

- 0.0 (1994.06): 로봇 배제 표준 지시자를 지원하는 마틴 코스터의 오리지널 robots.txt 메커니즘 
- 1.0 (1996.11): 웹 로봇 제어 방법 지시자ㅢ 지원이 추가된 마틴 코스터의 IETF 초안
- 2.0 (1996.11): 로봇 차단을 위한 확장 표준 정규식과 타이밍 정보를 포함한 숀 코너의 확장 널리 지원되지는 않는다.

### 9.4.2 웹 사이트와 robots.txt 파일들 

URL 을 방문하기전 웹 사이트에 robots.txt 파일이 존재한다면 로봇은 반드시 그 파일을 가져와야 한다.

- robots.txt 가져오기: robots.txt 파일이 존재한다면 서버는 그 파일은 text/plain 으로 반환한다. 404 상태 코드로 응답한다면 로봇의 접근을 제한하지 않는 것으로 간주하고 어떤 파일이든 요청하게 될 것 이다.
- 응답코드:
  - 200 으로 응답하면 로봇은 응답의 컨테츠를 파싱하여 규칙을 얻고 그 규칙을 따라야한다. 
  - 404 라면 규칙이 존재하지 않는다고 판단하고 제약없이 사이트에 접근한다. 
  - 401/403 으로 응답하면 접근은 완전히 제한되어 있다고 가정한다. 
  - 503 은 리소스 검색을 뒤로 미룬다. 
  - 3XX 리소스가 발견될 때 까지 리다이렉트한다.
  
### 9.4.3 robots.txt 파일 포맷

robots.txt 파일의 줄은 빈 줄, 주석 줄, 규칙 줄 세가지 종류가 있다. 규칙줄은 HTTP 헤더처럼 생겼고 (<필드>:<값>) 패턴 매칭을 위해 사용된다.

```text
User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow: 
```

- User-Agent 줄: 로봇의 레코드는 하나 이상의 User-Agent 줄로 시작한다. 





