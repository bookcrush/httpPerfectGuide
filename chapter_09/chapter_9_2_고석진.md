# HTTP

## 9.5 로봇 에티켓 
- 1. 신원 식별
  - 로봇의 신원을 밝히라: User-Agent 를 이용하여 웹 서버에게 로봇의 이름을 밝혀라
  - 기계의 신원을 밝혀라: DNS 엔트리를 가진 기계에서 실행된다는 것을 확실히 해서, 웹 사이트가 로봇의 IP 주소를 호스트 명을 통해 역방향 DNS 를 할 수 있도록 해라
  - 연락처를 밝혀라: HTTP 폼 필드를 사용해서 연락할 수 있는 이메일 주소를 제공하라
- 2. 동작 
  - 긴장하라: 로봇이 노련해지기 전까지는 운영자들을 고용해서 감시하라
  - 대비하라: 로봇이 여행을 떠나기전에 조직에 사실을 알려둬라
  - 감시와 로그: 로봇이 정상적으로 동작하는지 진행상황을 추적하고 기본적인 검사가 가능하도록 진단과 로깅 기능을 풍부하게 갖춰야한다.
  - 배우고 조정하라: 크롤링 할 때마다 새로운 것을 배우게 된다. 로봇을 조정하고 개선하라
- 3. 스스로를 제한하라
  - URL 을 필터링하라: 이해할 수 없거나 관심 없는 데이터를 참조하고 있다면 무시하는 것이 좋다.
  - 동적 URL 을 필터링하라: 보통 로봇은 동적인 게이트웨이로부터의 콘텐츠를 크롤링할 필요가 없다.
  - Accept 관련 헤더로 필터링: HTTP accept 관련 헤더들을 이용하여 콘텐츠를 이해할 수 있는지 말해주어야한다.
  - robots.txt 에 따르라: 방문 웹 사이트의 robots.txt 를 따라야한다.
  - 스스로를 억제하라: 한 사이트에 총 접근 횟수를 제한해야한다.
- 4. 루프와 중복을 견뎌내기, 그리고 그 외의 문제들
  - 모든 응답 코드 다루기: 모든 리다이렉트와 에러를 포함한 모든 HTTP 상태 코드를 다룰 수 있도록 준비되어 있어야한다.
  - URL 정규화하기: 모든 URL 을 표준화된 형식으로 변경하여 중복된 URL 을 제거하라 
  - 적극적으로 순환 피하기: 순환을 감지하고 피하기위해 노력하라 
  - 함정을 감시하라: 의도적이고 악의적인 순환을 피하기위해 함정을 감시하라
  - 블랙리스트를 관리하라: 함정, 깨진 사이트등을 블랙리스트로 추가하고 다시는 방문하지 않도록 해라 
- 5. 확장성
  - 대역폭 이해하기: 얼마나 많은 네트워크 대역폭이 사용 가능한지, 요구되는 시간에 로봇 작업을 끝마치는데 얼마나 필요할지 이해하라.
  - 시간 이해하기: 작업을 끝내는데 얼마나 많은 시간이 필요한지 이해하고, 소요된 시간이 추정한 것과 맞는지 간단히 검사해보라 
  - 분할 정복: 대규모 크롤링을 하는 상황에서는 멀티프로세서 서버든 서로 협력하는 여러 개의 작은 컴퓨터등 하드웨어들이 더 필요할 수 있다.
- 6. 신뢰성
  - 철저하게 테스트하라: 풀어놓기전 내부에서 철저하게 테스트하라
  - 체크포인트: 체크포인트/재시작 기능을 처음부터 설계하라 
  - 실패에 대한 유연성: 실패에 대비하여 로못을 실패가 발생했을 때도 계속 동작할 수 있도록 설계하라 
- 7. 소통
  - 준비하라: 로봇에 대한 정책 안내 페이지를 만들고 robots.txt 를 만드는 법에 대한 설명도 포함하라
  - 이해하라: 로봇 차단 규칙 표준에 대해 설명하고, 그래도 만족하지 못한다면 블랙리스트에 추가하라
  - 즉각 대응하라: 불만을 갖게되는 요소에 대해서 즉각 대응하라 
  
## 9.6 검색엔진

웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색엔진이다. 
웹 크롤러들은 먹이를 주듯 검색엔진에게 웹에 존재하는 문서들을 가져다 주어서, 검색엔진이 어떤 문서에 어떤 단어들이 존재하는지에 대한 색인을 생성할 수 있게 한다.

### 9.6.1 넓게 생각하라 

초창기에 검색엔진은 단순한 데이터베이스였다. 급격히 웹이 성장하면서 수십억개의 페이지에 접근을 해야하니 단순한 데이터베이스 형식으로는 동작이 어려워졌다.

